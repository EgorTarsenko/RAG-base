{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0682881f-044f-41ac-a0b4-ae4efc873649",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Initial Vector DB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc069d34-7dd9-4f72-8e9c-29ba357e42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89eb65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and set environment\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "os.environ['USER_AGENT'] = 'myagent'\n",
    "PROJECT_HOME = Path(os.environ.get('PROJECT_HOME', Path.cwd() / '..')).resolve()\n",
    "sys.path.append(str(PROJECT_HOME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.indexing.metadata import DocumentMetadata\n",
    "from app.databases.vector import VectorDB\n",
    "\n",
    "vector_db = VectorDB(\n",
    "    # auto_id=True,\n",
    "    # drop_old=True,  # Drop existing values inside the collection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cdbf1b-ed76-40b7-937c-f373a6b582e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea7566-f00a-400a-9e4e-c6a4a7ab136c",
   "metadata": {},
   "source": [
    "# Ingesting documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3737615-32da-477a-b26b-97f4f8553911",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_BASE_PATH = PROJECT_HOME / 'data'\n",
    "\n",
    "def get_documents_from_subfolder(subpath):\n",
    "    ''' Returns all the documents from a sub-path of the DOCS_BASE_PATH'''\n",
    "    return list((DOCS_BASE_PATH / subpath).glob(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948ae7d-b302-4505-9a91-5b722105665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tqdm.notebook import tqdm\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders.word_document import Docx2txtLoader\n",
    "\n",
    "def get_splits_from_paths(file_paths, loader):\n",
    "    '''\n",
    "    Returns langchain Documents split using a RecusriveCharacterTextSplitter (for now).\n",
    "    Their metadata is set to our project metadata.\n",
    "\n",
    "    loader needs to be some langchain loader, e.g. TextLoader.\n",
    "    '''\n",
    "    splits = []\n",
    "    for file_path in tqdm(file_paths, desc=\"Processing documents\"):\n",
    "        try:\n",
    "            loaded = loader(file_path)\n",
    "                        \n",
    "            docs = loaded.load()\n",
    "    \n",
    "            # Convert timestamp to formatted string\n",
    "            timestamp = file_path.stat().st_mtime\n",
    "            modified_date = datetime.fromtimestamp(timestamp)\n",
    "    \n",
    "            # Add metadata to all file chunks\n",
    "            for doc in docs:\n",
    "                metadata = DocumentMetadata(source_id= file_path.name,\n",
    "                                 source_name= file_path.name,\n",
    "                                 modified_at= modified_date)\n",
    "                doc.metadata = metadata.to_dict()\n",
    "    \n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "            splits += text_splitter.split_documents(docs)\n",
    "    \n",
    "        except Exception as exc:\n",
    "            print(f\"Error processing {file_path}: {exc}\")  # Optional: for debugging\n",
    "    return splits\n",
    "    \n",
    "def get_txt_splits_from_paths(file_paths): return get_splits_from_paths(file_paths, TextLoader)\n",
    "def get_pdf_splits_from_paths(file_paths): return get_splits_from_paths(file_paths, PyPDFLoader)\n",
    "def get_docx_splits_from_paths(file_paths): return get_splits_from_paths(file_paths, Docx2txtLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288e2f7-b2d9-4633-ad07-b3b56345a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_documents_from_subfolder(\"txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53124e7a-5f60-4c2b-bf40-440fb94e8d5a",
   "metadata": {},
   "source": [
    "## Ingesting txt documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c9ece-0eb6-4d36-aaa8-d56fbdadc862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes there is a subfolder of the /data/ folder called \"txt\".\n",
    "txt_splits = get_txt_splits_from_paths(get_documents_from_subfolder('txt'))\n",
    "txt_splits[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f850389-e258-4e04-8ce2-81ab17ff2f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.add_documents(documents=txt_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049c377-7a7d-47d9-81c1-f6ea9dbd52bc",
   "metadata": {},
   "source": [
    "## Ingesting PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016718f9-ee9b-4f3e-999f-88ac852784dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes there is a subfolder of the /data/ folder called \"pdf\".\n",
    "pdf_splits =  get_pdf_splits_from_paths(get_documents_from_subfolder(\"pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf8984-a775-40f5-a9fc-1eabdbae54ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_db.add_documents(documents=pdf_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d6b961-f7d9-456b-851d-68f854e46dba",
   "metadata": {},
   "source": [
    "## Ingesting Docx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79416b1-4f0f-4ffd-b995-27b0bb3b201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes there is a subfolder of the /data/ folder called \"docx\".\n",
    "docx_splits =  get_docx_splits_from_paths(get_documents_from_subfolder(\"docx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33bc90-307d-4a01-9e97-6e5bbf56beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.add_documents(documents=docx_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46113c94-b81c-4904-abc9-f9700db4b078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
