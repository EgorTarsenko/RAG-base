{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0682881f-044f-41ac-a0b4-ae4efc873649",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Initial Vector DB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc069d34-7dd9-4f72-8e9c-29ba357e42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from app.indexing.metadata import DocumentMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89eb65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and set environment\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "os.environ['USER_AGENT'] = 'myagent'\n",
    "PROJECT_HOME = Path(os.environ.get('PROJECT_HOME', Path.cwd() / '..')).resolve()\n",
    "sys.path.append(str(PROJECT_HOME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from app.databases.vector.milvus import Milvus\n",
    "from app.databases.vector import VectorDB\n",
    "\n",
    "vector_db = VectorDB(\n",
    "    # auto_id=True,\n",
    "    # drop_old=True,  # Drop existing values inside the collection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cdbf1b-ed76-40b7-937c-f373a6b582e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f13be-2319-4199-ad6c-d50b13471924",
   "metadata": {},
   "source": [
    "## Ingesting docx documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ee2c0-328a-402d-a3b5-47682aee66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Scan files and load them into the vector DB.\n",
    "\n",
    "# from langchain_community.document_loaders.word_document import Docx2txtLoader\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from tqdm.notebook import tqdm\n",
    "# from datetime import datetime\n",
    "\n",
    "# # File paths\n",
    "# docs_path = PROJECT_HOME / 'data' / 'docx'\n",
    "# # index_df = pd.read_csv(docs_path.parent / 'drive_files.csv')\n",
    "\n",
    "# # Scan all files that appear in the CSV.\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Specify your base directory containing the documents\n",
    "# docs_path = Path(docs_path)\n",
    "\n",
    "# # Get all .docx files in the directory\n",
    "# files = list(docs_path.glob(\"*.docx\"))\n",
    "\n",
    "# for file_path in tqdm(files, desc=\"Processing documents\"):\n",
    "#     try:\n",
    "#         loader = Docx2txtLoader(file_path)\n",
    "#         docs = loader.load()\n",
    "\n",
    "#         # Convert timestamp to formatted string\n",
    "#         timestamp = file_path.stat().st_mtime\n",
    "#         modified_date = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "#         # Add metadata to all file chunks\n",
    "#         for doc in docs:\n",
    "#             doc.metadata = doc.metadata | {\n",
    "#                 'source_name': file_path.name,\n",
    "#                 'modified_at': modified_date,  # Gets file modification time\n",
    "#                 'source_id': file_path.name,\n",
    "#             }\n",
    "\n",
    "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "#         splits = text_splitter.split_documents(docs)\n",
    "#         vector_db.add_documents(documents=splits)\n",
    "\n",
    "#     except Exception as exc:\n",
    "#         print(f\"Error processing {file_path}: {exc}\")  # Optional: for debugging\n",
    "\n",
    "\n",
    "# print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea7566-f00a-400a-9e4e-c6a4a7ab136c",
   "metadata": {},
   "source": [
    "# Ingesting documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3737615-32da-477a-b26b-97f4f8553911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "DOCS_BASE_PATH = PROJECT_HOME / 'data'\n",
    "\n",
    "def get_documents_from_subfolder(subpath):\n",
    "    ''' Returns all the documents from a sub-path of the DOCS_BASE_PATH'''\n",
    "    return list((DOCS_BASE_PATH / subpath).glob(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948ae7d-b302-4505-9a91-5b722105665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tqdm.notebook import tqdm\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders.word_document import Docx2txtLoader\n",
    "\n",
    "def get_splits_from_paths(file_paths, loader):\n",
    "    '''\n",
    "    Returns langchain Documents split using a RecusriveCharacterTextSplitter (for now).\n",
    "    Their metadata is set to our project metadata.\n",
    "\n",
    "    loader needs to be some langchain loader, e.g. TextLoader.\n",
    "    '''\n",
    "    splits = []\n",
    "    for file_path in tqdm(file_paths, desc=\"Processing documents\"):\n",
    "        try:\n",
    "            loader = loader(file_path)\n",
    "            docs = loader.load()\n",
    "    \n",
    "            # Convert timestamp to formatted string\n",
    "            timestamp = file_path.stat().st_mtime\n",
    "            modified_date = datetime.fromtimestamp(timestamp)# .strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "            # Add metadata to all file chunks\n",
    "            for doc in docs:\n",
    "                metadata = DocumentMetadata(source_id= file_path.name,\n",
    "                                 source_name= file_path.name,\n",
    "                                 modified_at= modified_date)\n",
    "                doc.metadata = metadata.to_dict()\n",
    "    \n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "            splits += text_splitter.split_documents(docs)\n",
    "            # vector_db.add_documents(documents=splits)\n",
    "    \n",
    "        except Exception as exc:\n",
    "            print(f\"Error processing {file_path}: {exc}\")  # Optional: for debugging\n",
    "    return splits\n",
    "    \n",
    "def get_txt_splits_from_paths(file_paths): return get_splits_from_paths(file_paths, TextLoader)\n",
    "def get_pdf_splits_from_paths(file_paths): return get_splits_from_paths(file_paths, PyPDFLoader)\n",
    "def get_docx_splits_from_paths(file_paths): return get_splits_from_paths(file_paths, Docx2txtLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288e2f7-b2d9-4633-ad07-b3b56345a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_documents_from_subfolder(\"txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53124e7a-5f60-4c2b-bf40-440fb94e8d5a",
   "metadata": {},
   "source": [
    "## Ingesting txt documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c9ece-0eb6-4d36-aaa8-d56fbdadc862",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_splits = get_txt_splits_from_paths(get_documents_from_subfolder('txt'))\n",
    "txt_splits[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f850389-e258-4e04-8ce2-81ab17ff2f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.add_documents(documents=txt_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049c377-7a7d-47d9-81c1-f6ea9dbd52bc",
   "metadata": {},
   "source": [
    "## Ingesting PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016718f9-ee9b-4f3e-999f-88ac852784dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_splits_alice = get_pdf_splits_from_paths(get_documents_from_subfolder(\"pdf_alice\"))\n",
    "pdf_splits_test =  get_pdf_splits_from_paths(get_documents_from_subfolder(\"pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf8984-a775-40f5-a9fc-1eabdbae54ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vector_db.add_documents(documents=pdf_splits_test)\n",
    "vector_db.add_documents(documents=pdf_splits_alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d6b961-f7d9-456b-851d-68f854e46dba",
   "metadata": {},
   "source": [
    "## Ingesting Docx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79416b1-4f0f-4ffd-b995-27b0bb3b201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_splits =  get_docx_splits_from_paths(get_documents_from_subfolder(\"docx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33bc90-307d-4a01-9e97-6e5bbf56beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.add_documents(documents=docx_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69efd23-3d8f-488d-8615-5526fcde8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Scan files and load them into the vector DB.\n",
    "\n",
    "# from langchain_community.document_loaders.text import TextLoader\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from tqdm.notebook import tqdm\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "# # Get all .docx files in the directory\n",
    "# files = get_documents_from_subfolder(\"txt\")\n",
    "\n",
    "# for file_path in tqdm(files, desc=\"Processing documents\"):\n",
    "#     try:\n",
    "#         loader = TextLoader(file_path)\n",
    "#         docs = loader.load()\n",
    "\n",
    "#         # Convert timestamp to formatted string\n",
    "#         timestamp = file_path.stat().st_mtime\n",
    "#         modified_date = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#         # Add metadata to all file chunks\n",
    "#         for doc in docs:\n",
    "#             doc.metadata = doc.metadata | {\n",
    "#                 'source_name': file_path.name,\n",
    "#                 'modified_at': modified_date,  # Gets file modification time\n",
    "#                 'source_id': file_path.name,\n",
    "#             }\n",
    "\n",
    "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "#         splits = text_splitter.split_documents(docs)\n",
    "#         vector_db.add_documents(documents=splits)\n",
    "\n",
    "#     except Exception as exc:\n",
    "#         print(f\"Error processing {file_path}: {exc}\")  # Optional: for debugging\n",
    "\n",
    "\n",
    "# print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d4908-5500-4176-8d9a-43d9ac1c8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_documents_from_subfolder(\"txt\")\n",
    "\n",
    "loader = TextLoader(files[0])\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e8f8a-e60b-4502-a91c-60a17b02434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48577050-4f05-41e0-bfeb-fbc94c02dce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e8d90-3fe0-4102-b942-259f4b68c9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fddced-c394-43b9-bf5a-f2445e16ee8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420fb3a-52c5-4133-8786-f9e90722e0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc222fa-312a-418a-a728-0021f46a16d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c4d51-49f2-4a8c-b6bf-6328d0bc7eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252122b9-9ce4-49fe-90ee-46a73cde5075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971a710-1d37-49a0-b586-b7b4ee1bf984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f06fe-3993-46e4-95ce-af01fa67d4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c3d21-eb97-4e65-adf6-347a60c2f44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "300351a8-1190-4cea-846f-4c28a02d1c8e",
   "metadata": {},
   "source": [
    "## Ingesting PDFs OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07de550-77f2-4462-8c42-829401523f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff702e-fd47-4165-8ea7-ac93e32c15b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_splits_from_paths(files):\n",
    "    '''\n",
    "    Returns langchain Documents split using a RecusriveCharacterTextSplitter (for now).\n",
    "    Their metadata is set to our project metadata.\n",
    "    '''\n",
    "    splits = []\n",
    "    for file_path in tqdm(files, desc=\"Processing documents\"):\n",
    "        try:\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            docs = loader.load()\n",
    "    \n",
    "            # Convert timestamp to formatted string\n",
    "            timestamp = file_path.stat().st_mtime\n",
    "            modified_date = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "            # Add metadata to all file chunks\n",
    "            # for doc in docs:\n",
    "            for doc in docs[:1]: # Temp - only do 1!\n",
    "                doc.metadata =  {\n",
    "                    'source_name': file_path.name,\n",
    "                    'modified_at': modified_date,  # Gets file modification time\n",
    "                    'source_id': file_path.name,\n",
    "                }\n",
    "    \n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "            splits += text_splitter.split_documents(docs)\n",
    "            # vector_db.add_documents(documents=splits)\n",
    "    \n",
    "        except Exception as exc:\n",
    "            print(f\"Error processing {file_path}: {exc}\")  # Optional: for debugging\n",
    "    return splits\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c156c07-2035-4e8b-a651-bd2b00d1d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_splits = get_pdf_splits_from_paths(get_documents_from_subfolder('pdf_alice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcd3e0b-e793-4a7d-a436-b77019ede47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_splits = alice_splits = get_pdf_splits_from_paths(get_documents_from_subfolder('pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976bfd5-cc13-4cb0-b66e-ca7e794ece77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_splits, alice_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f276c0f-5bcc-40df-9229-5007db7e7cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scan files and load them into the vector DB.\n",
    "# files = get_documents_from_subfolder('pdf_alice')\n",
    "\n",
    "# for file_path in tqdm(files, desc=\"Processing documents\"):\n",
    "#     try:\n",
    "#         loader = PyPDFLoader(file_path)\n",
    "#         docs = loader.load()\n",
    "\n",
    "#         # Convert timestamp to formatted string\n",
    "#         timestamp = file_path.stat().st_mtime\n",
    "#         modified_date = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#         # Add metadata to all file chunks\n",
    "#         for doc in docs[:1]:\n",
    "#             doc.metadata = doc.metadata | {\n",
    "#                 'source_name': file_path.name,\n",
    "#                 'modified_at': modified_date,  # Gets file modification time\n",
    "#                 'source_id': file_path.name,\n",
    "#             }\n",
    "\n",
    "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "#         splits = text_splitter.split_documents(docs)\n",
    "#         # vector_db.add_documents(documents=splits)\n",
    "\n",
    "#     except Exception as exc:\n",
    "#         print(f\"Error processing {file_path}: {exc}\")  # Optional: for debugging\n",
    "\n",
    "\n",
    "# print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527786e-5084-4267-9f40-2f634d4493e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_documents_from_subfolder('pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7eee3d-4dc7-46ed-b596-0782005b5690",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(files[0])\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b8000-1d6e-41fb-ae9b-2aaa749a5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315a4bd9-0310-4a1d-90d9-894cf4bcbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del docs[0].metadata['page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33cac70-5ae3-4f25-8606-8f0ce8f78f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0215aaa-062f-498d-80ca-9f54dff5a241",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6be1db-7ca9-4f26-8c79-c072c83df94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29224d1-3ac4-424b-ae87-ce4b10b834de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits[:1]\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content='hello world',\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352c935-19d6-4c33-8a18-9828d5ff28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e800a52-ed4a-4276-8075-7a44f84e142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.add_documents([document_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449138dd-6c42-4739-b200-b8b33c94e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "document_2 = Document(\n",
    "    page_content=splits[0].page_content,\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "document_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d958e30-724e-4704-a704-1adac41eab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8022f63-dfcb-471b-bcd4-980ed2fd7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d6319-4913-41b6-8137-1e785f8721d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
