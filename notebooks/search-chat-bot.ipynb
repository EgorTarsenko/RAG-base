{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1788dd9a-ff33-435d-b6db-e9d413127a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e325d6f-4f5e-4df9-a62b-70a4b151427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and set environment\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "os.environ['USER_AGENT'] = 'myagent'\n",
    "PROJECT_HOME = Path(os.environ.get('PROJECT_HOME', Path.cwd() / '..')).resolve()\n",
    "sys.path.append(str(PROJECT_HOME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f11e38-69d5-4cbb-9ae2-de9538fe82d9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b3183-40d1-4760-b5ae-464b2e4a74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the vector DB as a retriever.\n",
    "# This is the place to define the search algorithm, parameters and how many values (`k`) to return per query.\n",
    "\n",
    "from app.databases.milvus import Milvus\n",
    "\n",
    "vector_db = Milvus()\n",
    "retriever = vector_db.as_retriever(search_kwargs={'k': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64261d70-f064-4174-abe6-d6f840039cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# The ChatBot LLM\n",
    "llm = ChatBedrock(\n",
    "    model_id='anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
    "    region_name='us-east-1',\n",
    "    model_kwargs=dict(temperature=0),\n",
    ")\n",
    "\n",
    "# Retriever tool, for the R in RAG.\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    'Internal_Company_Info_Retriever',\n",
    "    'Searches and retrieves data from the corpus of documents that the company has',\n",
    "\n",
    "    # Controls how the returned results will look when passed to the LLM.\n",
    "    # To see available `variables`, check the `retriever.invoke('some query')[0].metadata.keys()`\n",
    "    document_prompt=PromptTemplate(input_variables=['source', 'page_content'], template='\\n'.join(['Source:', '{source}', 'Content:', '{page_content}'])),\n",
    "    document_separator='\\n\\n========\\n\\n',\n",
    ")\n",
    "tools = [tool]\n",
    "\n",
    "# Store conversation history. In prod, should replace with a real DB.\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "# The prompt message to use with the LLM. Provide clear and concise instructions to the LLM.\n",
    "prompt_message = SystemMessage(\"\"\"When answering the user question using data from the tools, be sure to:\n",
    "1. First list the sources you are going to use in your answer. The list of sources should be of the form:\n",
    "[1] - full/path/to/file (index of document in the retriever's answer (e.g. 0, 4): <The quote that being used>\n",
    "[2] - ...\n",
    "2. Add a separator '\\n\\n+++++++++++++\\n\\n'\n",
    "3. Write a concise answer based only on the sources and quotes you listed above.\n",
    "\n",
    "If you don't know the answer, answer that you don't know based on the information you have. If you're not sure, state that you're not sure.\"\"\")\n",
    "\n",
    "# Create the agent itself.\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory, state_modifier=prompt_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91374022-6f7b-4ba2-859d-5ada74675565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# To start a new conversation, change the `thread_id` to a new value.\n",
    "config = {\"configurable\": {\"thread_id\": \"user_12837\"}}\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Make a pretty separator for the next human message\n",
    "    HumanMessage('').pretty_print()\n",
    "\n",
    "    # Get user question.\n",
    "    question = input('Write a question to the LLM. Write `q` or `quit` to exit:')\n",
    "\n",
    "    # System output\n",
    "    SystemMessage('').pretty_print()\n",
    "    \n",
    "    if question.lower() in ('q', 'quit'):\n",
    "        # Quit\n",
    "        print('Quitting for now. You can continue the conversation by rerunning this cell')\n",
    "        break\n",
    "\n",
    "    print('Processing question')\n",
    "    \n",
    "    # Ask the LLM.\n",
    "    for s in agent_executor.stream({\"messages\": [HumanMessage(content=question)]}, config=config):\n",
    "        if 'agent' in s and s['agent']['messages'][-1] and getattr(s['agent']['messages'][-1], 'tool_calls', None):\n",
    "            print('Searching Vector DB for data')\n",
    "        elif 'tools' in s:\n",
    "            print('Analyzing retrieved data for answers')\n",
    "\n",
    "    s['agent']['messages'][0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c759e2-476a-487a-88bf-9c2167ac1c79",
   "metadata": {},
   "source": [
    "----\n",
    "#### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b21523e-020d-4752-8518-a248a163d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's you see more metadata about the conversation.\n",
    "state = agent_executor.get_state(config)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa362317-2ba8-47e6-a758-cd53ff904539",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e432b-edd3-4a9a-b674-a7c337b08727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
