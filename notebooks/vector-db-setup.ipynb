{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc069d34-7dd9-4f72-8e9c-29ba357e42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89eb65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and set environment\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "os.environ['USER_AGENT'] = 'myagent'\n",
    "PROJECT_HOME = Path(os.environ.get('PROJECT_HOME', Path.cwd() / '..')).resolve()\n",
    "sys.path.append(str(PROJECT_HOME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac5bc7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.databases.milvus import Milvus\n",
    "\n",
    "vector_db = Milvus(\n",
    "    # auto_id=True,\n",
    "    # drop_old=True,  # Drop existing values inside the collection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd58c49c-32b2-451a-9833-fb585f9c5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan files and load them into the vector DB.\n",
    "\n",
    "from langchain_community.document_loaders.word_document import Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# File paths\n",
    "docs_path = PROJECT_HOME / 'data' / 'gdrive-files'\n",
    "index_df = pd.read_csv(docs_path.parent / 'drive_files.csv')\n",
    "\n",
    "# Scan all files that appear in the CSV.\n",
    "for idx, (_, file_rec) in tqdm(enumerate(index_df.iterrows()), total=len(index_df)):\n",
    "\n",
    "    try:\n",
    "        fs_file_path = docs_path / f'{file_rec[\"ID\"]}_{file_rec[\"Name\"]}'\n",
    "\n",
    "        # Some files are missing a `.docx` in the `Name` column.\n",
    "        try:\n",
    "            loader = Docx2txtLoader(fs_file_path)\n",
    "        except ValueError:\n",
    "            fs_file_path = Path(str(fs_file_path) + '.docx')\n",
    "            loader = Docx2txtLoader(fs_file_path)\n",
    "\n",
    "        docs = loader.load()\n",
    "\n",
    "        # Add metadata to all file chunks\n",
    "        for doc in docs:\n",
    "            doc.metadata = doc.metadata | {\n",
    "                'source_id': file_rec['URL'],\n",
    "                'source_name': file_rec['Name'],\n",
    "                'modified_at': file_rec['Modified Time'],\n",
    "            }\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "        vector_db.add_documents(documents=splits)\n",
    "\n",
    "    except Exception as exc:\n",
    "        # If a file failed for some reason, just print the path and continue to the next file.\n",
    "        # We can't stop everything for every failure.\n",
    "        print(fs_file_path, flush=True)\n",
    "        print(exc, flush=True)\n",
    "        continue\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94dd13-d261-47be-bb62-86f7a564612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scan files and load them into the vector DB.\n",
    "\n",
    "# from langchain_community.document_loaders.word_document import Docx2txtLoader\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# docs_path = PROJECT_HOME / 'data' / \n",
    "# for file_path in tqdm(docs_path.glob('**/*.docx')):\n",
    "#     loader = Docx2txtLoader(file_path)\n",
    "#     docs = loader.load()\n",
    "\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "#     splits = text_splitter.split_documents(docs)\n",
    "#     vector_db.add_documents(documents=splits)\n",
    "\n",
    "# print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635854d-59da-40ed-867e-287565128050",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987085cd-c7cf-4167-b4d9-16e517b1b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a977ba1-f1de-4985-9cd6-bc3b9e86d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.invoke('onelogin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea31eb-fdb1-4713-bf45-75068148128f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14f2074b-69ab-4237-8fab-7148cd456274",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "## Adding a Text Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e69fe-7f38-4a89-9fa9-aa5f779a5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import text_to_docs\n",
    "# from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a1707-9ddf-44c0-8287-c00afa544584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [\n",
    "#     'The recommended headphones to use while listening to music are BoseQC35',\n",
    "#     'The recommended headphones to use while listening to podcasts are Airpods Pro',\n",
    "# ]\n",
    "\n",
    "# docs = [Document(page_content=text, metadata={\n",
    "#     # 'tags': ['headphones', 'music'],\n",
    "#     'modified_at': '2024-09-22',\n",
    "#     'source_id': 'NA',\n",
    "#     'source_name': 'NA',\n",
    "# }) for text in texts]\n",
    "\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52897735-921c-4f10-8061-551b908327cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304f346-7c22-4021-93e7-7d94c80093e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# res = vector_db.add_documents(documents=splits)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0371e3c-f462-470a-a955-2b545c5ae833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.invoke('headphones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63555bf0-6b3b-4bf7-bb29-aa0dfc471a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deleting a document from the DB\n",
    "# res = vector_db.delete(expr='source_id like \"%NA%\"')\n",
    "# type(res), res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f636f5-83fd-4b62-a27b-7f4356235a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
